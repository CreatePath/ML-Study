{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPh+J33f1B8a8xFn/TlQSn1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Stochastic Gradient Descent (SGD)\n","손실함수의 기울기가 0으로 수렴하는 방향으로 가중치들을 업데이트하는 기법.   \n","$W_{update} = W_{curr} - k(dL/dW)$    \n","'W에 대한 편미분' * '학습률' 만큼 이동."],"metadata":{"id":"qahLF54qo0hw"}},{"cell_type":"markdown","source":["## 단점\n","**loss function이 anisotropy function일 경우, 최솟값에 다다르지 못하거나, 비효율적으로 움직일 수 있음.**   \n","anisotropy function: 방향에 따라 성질이 달라지는 함수. 여기서는 축의 방향에 따라 기울기의 성질이 달라진다는 것.   \n","ex. y축에 따른 기울기는 크게 변하지만, x축에 따른 기울기는 크게 변하지 않는 경우."],"metadata":{"id":"iWtwfVTBqBGl"}},{"cell_type":"markdown","source":["# Momentum\n","$v_{update} = av_{curr} - k(dL/dW)$ (d라고 썼지만 편미분이다)   \n","$W_{update} = W_{curr} + v$"],"metadata":{"id":"v_-zhMlUrBxp"}},{"cell_type":"markdown","source":["SGD 방식에 $av$를 더해줌으로써 손실함수 기울기가 작아도 비교적 많이 움직이도록 유도한다.   \n","비유하자면 저항같은 개념?   \n","그릇에서 굴러가는 공의 모습을 떠올리면 됨."],"metadata":{"id":"G9KDk0aDrqsl"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"ibpW9UgVuxXF","executionInfo":{"status":"ok","timestamp":1672147900457,"user_tz":-540,"elapsed":5,"user":{"displayName":"이수현","userId":"13832946259713849753"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"HoOaACa0oozl","executionInfo":{"status":"ok","timestamp":1672147900458,"user_tz":-540,"elapsed":6,"user":{"displayName":"이수현","userId":"13832946259713849753"}}},"outputs":[],"source":["class Momentum:\n","  def __init__(self, lr=0.01, momentum = 0.9):\n","    self.lr = lr\n","    self.momentum = momentum\n","    self.v = None\n","\n","  def update(self, params, grads):\n","    if self.v is None:\n","      self.v = {}\n","      for key, val in params.items():\n","        self.v[key] = np.zeros_like(val)\n","\n","    for key, val in params.items():\n","      self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n","      params[key] += self.v[key]"]},{"cell_type":"markdown","source":["# AdaGrad\n","learning rate를 서서히 감소시키는 기법.   \n","learning rate가 너무 크면 under fitting이 발생하고, 반대로 너무 작으면 학습 시간이 길어진다.   \n","학습시키면서 그때그때 적절한 learning rate를 찾아주는 기법이라고 보면 된다."],"metadata":{"id":"5hIuTeL-vkk9"}},{"cell_type":"markdown","source":["$h_{update} = h_{curr} + dL/dW (o) dL/dW$ ((o)는 행렬 원소끼리 곱. 즉 브로드캐스트 연산)   \n","$W_{update} = W_{new} - k(dL/dW)/sqrt(h)$    \n","learing rate $k$가 $sqrt(h)$배 만큼 줄어든다. "],"metadata":{"id":"RG3yjDKVwFKQ"}},{"cell_type":"markdown","source":["## RMSProp\n","AdaGrad에서 어느 순간 갱신량이 0이 되는 걸 극복한 기법.    \n","필요에 따라 먼 과거의 기울기와 새로운 기울기의 영향력을 조절하는 방식.    \n","Exponential Moving Average(EMA, 지수이동평균)도 찾아보자."],"metadata":{"id":"ZXklW0RPxIql"}},{"cell_type":"code","source":["class AdaGrad:\n","  def __init__(self, lr=0.01):\n","    self.lr = lr\n","    self.h = None\n","\n","  def update(self, params, grads):\n","    if self.h is None:\n","      self.h = {}\n","      for key, val in params.itmes():\n","        self.h[key] = np.zeros_like(val)\n","\n","    for key, val in params.itmes():\n","      self.h[key] += grads[key] * grads[key]\n","      params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)"],"metadata":{"id":"UfyRq2rCtci7","executionInfo":{"status":"ok","timestamp":1672147900458,"user_tz":-540,"elapsed":5,"user":{"displayName":"이수현","userId":"13832946259713849753"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Adam\n","저자 피셜 Momentum + AdaGrad 라는데, 정확한건 논문을 찾아보자.   \n","코드는 책의 github 코드 참고"],"metadata":{"id":"Gl7uXjiwzm_Q"}}]}